
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Choosing between prediction and causal inference &#8212; Birth Outcomes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=888ff710"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=afe5de03"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script src="https://unpkg.com/mermaid@10.2.0/dist/mermaid.min.js"></script>
    <script>mermaid.initialize({startOnLoad:true});</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'causal_concepts/design_intro_2_predict_vs_causal';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Causal inference" href="design_intro_3_causal.html" />
    <link rel="prev" title="The treatment paradox" href="design_intro_1_treatment_paradox.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Birth Outcomes</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Birth outcomes summary
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../background/background.html">Background</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../background/stages.html">Stages of pregnancy and birth</a></li>
<li class="toctree-l2"><a class="reference internal" href="../background/interventions.html">Interventions during labour</a></li>
<li class="toctree-l2"><a class="reference internal" href="../background/research_groups.html">Research groups</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../outcomes/outcomes.html">Identifying hypoxic ischaemic encephalopathy (HIE)</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_background.html">Examples of identifying HIE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_summary.html">Summary of possible indicators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_cooling.html">Therapeutic hypothermia</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_hie_diagnosis.html">Diagnostic codes for HIE</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_blood.html">Umbilical cord blood gas analysis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_sentinel.html">Sentinel events</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_resus.html">Resuscitation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_meconium.html">Meconium-stained amniotic fluid</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_care.html">Transfer to neonatal care services</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_cp.html">Cerebral palsy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_mri.html">Neuroimaging evidence</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_enceph_symptoms.html">Symptoms of neonatal encephalopathy (NE)</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_apgar.html">Apgar scores</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_death.html">Death</a></li>
<li class="toctree-l2"><a class="reference internal" href="../outcomes/neo_out_ctg.html">Abnormal CTG</a></li>

</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="intro.html">Study design - causal concepts</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="design_intro_1_treatment_paradox.html">The treatment paradox</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Choosing between prediction and causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_intro_3_causal.html">Causal inference</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_counterfactuals.html">Counterfactuals</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_dag_1.html">Directed acyclic graphs (DAGs) #1 (basics)</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_dag_2.html">DAGs #2 (advanced concepts)</a></li>
<li class="toctree-l2"><a class="reference internal" href="design_sem.html">Structural equation modelling</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../design_methods/intro.html">Study design - causal methods</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_causal_assumptions.html">Causal assumptions</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_causal_estimands.html">Causal estimands</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_target_trial.html">Target trial emulation</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_observational.html">Observational study designs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_methods_1_intro.html">Methods #1: Introduction to methods to adjust for confounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_methods_2_conventional.html">Methods #2: Conventional approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_methods_3_gmethods.html">Methods #3: G-methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_methods_4_unobserved.html">Methods #4: Methods for unobserved confounding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_methods_5_other.html">Methods #5: Other</a></li>
<li class="toctree-l2"><a class="reference internal" href="../design_methods/design_other.html">Other design considerations</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../examples/intro.html">Examples</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../examples/obstetric_examples.html">Obstetric studies that address the treatment paradox</a></li>
<li class="toctree-l2"><a class="reference internal" href="../examples/instrumental_variables.html">Instrumental variables</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../our_study/design_plan.html">Our study</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../appendices/intro.html">Appendices</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="../our_study/our_dag.html">HIE DAG</a></li>
<li class="toctree-l2"><a class="reference internal" href="../our_study/our_target_trial_protocol.html">HIE target trial protocol</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendices/neo_out_enceph_treatment_outcomes.html">Challenge 2: Outcomes for trials</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendices/neo_out_chorio.html">More on chorioamnionitis</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendices/fetal_risk_factors.html">Fetal risk factors</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendices/parental_outcomes.html">Parental outcomes</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendices/routine_data.html">Routinely collected data</a></li>
<li class="toctree-l2"><a class="reference internal" href="../appendices/key_papers.html">Key Papers</a></li>
</ul>
</li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fcausal_concepts/design_intro_2_predict_vs_causal.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/causal_concepts/design_intro_2_predict_vs_causal.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Choosing between prediction and causal inference</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-know-whether-you-are-interested-in-prediction-or-causation">How do you know whether you are interested in prediction or causation?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrative-example">Illustrative example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explainability-v-s-causality">Explainability v.s. causality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doing-predictive-and-etiological-research">Doing predictive AND etiological research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-can-prediction-models-answer-causal-questions">When can prediction models answer causal questions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-and-controversy">Confusion and controversy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasons-for-confusion">Reasons for confusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-in-the-literature">Confusion in the literature</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepts-and-principles">Concepts and principles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ladder-of-causality">Ladder of causality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-inference">Types of inference</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="choosing-between-prediction-and-causal-inference">
<h1>Choosing between prediction and causal inference<a class="headerlink" href="#choosing-between-prediction-and-causal-inference" title="Link to this heading">#</a></h1>
<div class="info admonition">
<p class="admonition-title">Executive summary</p>
<p>Predictive research aims to predict an outcome with the <strong>best accuracy</strong>. Explainability (e.g. SHAP) is about understanding why a model makes certain predictions. When making predictions, whether the direction of relationships (e.g. from SHAP values) is true/causal doesn’t matter, as the goal is just to make the best predictions.</p>
<p>Etiological research aims to <strong>uncover causal effects</strong>. It involves finding an unbiased estimate of the effect of X on Y, by controlling for confounding factors that could bias the estimate. In causal inference, the true direction of relationships (and the counterfactual scenarios) are important. It typically starts with drawing a causal diagram.</p>
</div>
<section id="how-do-you-know-whether-you-are-interested-in-prediction-or-causation">
<h2>How do you know whether you are interested in prediction or causation?<a class="headerlink" href="#how-do-you-know-whether-you-are-interested-in-prediction-or-causation" title="Link to this heading">#</a></h2>
<p>Scientific research can be categorised into descriptive, predictive and etiological research. Descriptive research aims to summarise the characteristics of a group (or person).<a class="reference external" href="https://doi.org/10.1016/j.dcn.2020.100867">[Hamaker et al. 2020]</a> However, this page focusses just on <strong>predictive and etiological research</strong>.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p></p></th>
<th class="head"><p><strong>Predictive research</strong></p></th>
<th class="head"><p><strong>Etiological research</strong> (or “explanatory” research)</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Aim</p></td>
<td><p>Aims to predict an outcome with the <strong>best accuracy</strong>.<a class="reference external" href="https://link.springer.com/article/10.1007/s10654-021-00794-w">[Ramspek et al. 2021]</a></p></td>
<td><p>Aims to uncover <strong>causal effects</strong> - i.e. <strong>causal inference</strong>.</p></td>
</tr>
<tr class="row-odd"><td><p>Relationships</p></td>
<td><p>It doesn’t matter whether predictors are causal or not - <strong>just focussed on best prediction</strong>.</p></td>
<td><p>Concerned with the <strong>true causal relationships</strong> between variables.</p></td>
</tr>
<tr class="row-even"><td><p>Directionality</p></td>
<td><p>We’re interested in <strong>associations</strong> (i.e. relationships aren’t directional).<a class="reference external" href="https://stats.stackexchange.com/questions/56909/what-is-the-relation-between-causal-inference-and-prediction">[source]</a></p></td>
<td><p>It is important that relationships are <strong>directional</strong>, as these directions are required to support interventional reasoning.<a class="reference external" href="https://stats.stackexchange.com/questions/56909/what-is-the-relation-between-causal-inference-and-prediction">[source]</a></p></td>
</tr>
</tbody>
</table>
<section id="illustrative-example">
<h3>Illustrative example<a class="headerlink" href="#illustrative-example" title="Link to this heading">#</a></h3>
<p>A team have built an XGBoost model to predict whether customers will renew their subscription. They use SHAP values to understand how the model made its predictions. They notice a suprising finding: <strong>users who report more bugs are more likely to renew. Is this a problem? It depends on their goal</strong></p>
<ul class="simple">
<li><p>If their goal is to <strong>predict customer retention to estimate future revenue</strong>, then this relationship is helpful for <strong>prediction</strong>, and it doesn’t matter about the direction, as long as our predictions are good.</p></li>
<li><p>However, if their goal is to <strong>inform actions to help retain customers</strong>, then it is important to understand the true relationships between features and the outcomes, and the counterfactual scenarios if features were modified. In this case, the team are interested in <strong>causation</strong>. In order for the team to understand the causal relationships, they would need to use causal inference methods (causal diagrams, appropriate techniques to account for confounding).</p></li>
</ul>
<p><strong>Why did this finding occur?</strong> If the team are interested in causation, they could draw a causal diagram (simplified version below). In it, they notice that some features are influenced by unmeasured confounding. WIth the example above, <strong>users who report more bugs are people who use the product more so encounter more bugs, but need the product more so are more likely to report</strong>. Because they can’t directly measure product need, the correlation they end up capturing in the predict model between bugs reported and renewal combines a small negative direct effect of bugs faced and a large positive confounding effect from product need. <a class="reference external" href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html">[source]</a></p>
<div class="mermaid">
              flowchart LR;

    need(&quot;&lt;b&gt;Unmeasured&lt;/b&gt;&lt;br&gt;Product need&quot;):::white;
    month(&quot;Monthly usage&quot;):::white;
    face(&quot;&lt;b&gt;Unmeasured&lt;/b&gt;&lt;br&gt;Bugs faced&quot;):::white;
    report(&quot;Bugs reported&quot;):::white;
    ren(&quot;Did renew&quot;):::important;


    month --&gt; face; face --&gt; report; face --&gt; ren;
    need --&gt; report;
    need --&gt; month;
    need --&gt; ren;
    month --&gt; ren;

    classDef white fill:#FFFFFF, stroke:#FFFFFF
    classDef black fill:#FFFFFF, stroke:#000000
    classDef important fill:#DDF2D1, stroke: #FFFFFF;
        </div></section>
<section id="explainability-v-s-causality">
<h3>Explainability v.s. causality<a class="headerlink" href="#explainability-v-s-causality" title="Link to this heading">#</a></h3>
<p>Explainability refers to being able to <strong>understand why a model makes certain predictions</strong>. The aim of explainable AI is to make ML models more transparent. It provides insights on:</p>
<ul class="simple">
<li><p>How a model makes predictions</p></li>
<li><p>What features are most important</p></li>
<li><p>How sensitive a model is to changes in the input <a class="reference external" href="https://doi.org/10.1038/s42256-020-0197-y">[Prosperi et al. 2020]</a></p></li>
</ul>
<p>The contribution of individual covariates are often mistakenly interpreted causally, but the methods used were focused on combining covariates to optimise predictive accuracy, and not to predict the outcome distribution under hypothetical interventions. <a class="reference external" href="https://doi.org/10.1186/s41512-021-00092-9">[Lin et al. 2021]</a></p>
<p>However, it cannot be used to infer causal relationships, since the findings may be biased by stratification or unmeasured confounders, or mediated by other factors in the causal pathway.<a class="reference external" href="https://doi.org/10.1038/s42256-020-0197-y">[Prosperi et al. 2020]</a> If you wish to make claims about causality, you will need to build a casual model. Causal ML aims to <strong>infer causal relationships</strong> from observational data by estimating the effect of a specific variable on the outcome, while <strong>appropriately controlling</strong> for other confounding factors that could bias the estimate.<a class="reference external" href="https://medium.com/&#64;dahnert.sebastian/understand-the-difference-and-intersection-between-causal-ml-and-explainable-ai-65583132e704">[source]</a></p>
</section>
</section>
<section id="doing-predictive-and-etiological-research">
<h2>Doing predictive AND etiological research<a class="headerlink" href="#doing-predictive-and-etiological-research" title="Link to this heading">#</a></h2>
<p>Many problems will require a <strong>combination of prediction and causation</strong>.</p>
<ul class="simple">
<li><p>“Pure forecasting task” - e.g. just want to predict whether or not it will rain, and don’t care why/what caused the rain</p></li>
<li><p>“Pure causation task” - e.g. performing a rain dance presumed to save dying crops, only if it actually causes rain</p></li>
<li><p>Combination of the two - e.g. if planning assignment of fire inspectors across a city, should (a) predict will establishment will be in violation of fire codes, and (b) estimate causal effect on establishment’s behaviour of receiving an inspection or not</p></li>
</ul>
<p>Beck et al. 2018 also argue that <strong>prediction remains relevant even if you’re only interested in understanding causal effects</strong>. Explanations that invoke causal mechanisms always make predictions - specifically, predictions about what will happen under an intervention. ‘Whether they do so explicitly or not, that is, causal claims necessarily make predictions; thus it is both fair and arguably useful to hold them accountable for the accuracy of the predictions they make.’ They therefore argue that the <strong>predictive performance of models and of explanations</strong> is important to include (e.g. R<sup>2</sup>, MAE, RMSE, AUC, accuracy, recall, F1).<a class="reference external" href="https://doi.org/10.31219/osf.io/u6vz5">[Beck et al. 2018]</a></p>
</section>
<section id="when-can-prediction-models-answer-causal-questions">
<h2>When can prediction models answer causal questions?<a class="headerlink" href="#when-can-prediction-models-answer-causal-questions" title="Link to this heading">#</a></h2>
<p>As this example is from a simulation study where know true causal effects, we can plot the SHAP values from the prediction models v.s. the known true causal effects.</p>
<p><img alt="Causal effects" src="../_images/shap_bugs_causal_vs_shap.png" /></p>
<p>We can also add clustering to see the redundancy structure of the data as a dendrogram - ‘when features merge together at the bottom (left) of the dendrogram it means that that the information those features contain about the outcome (renewal) is very redundant and the model could have used either feature. When features merge together at the top (right) of the dendrogram it means the information they contain about the outcome is independent from each other.’</p>
<p><img alt="Redundancy" src="../_images/shap_bugs_redundancy.png" /></p>
<p><strong>When can predictive models answer causal questions?</strong> When the feature is independent of (a) other features in the model, and (b) unobserved confounders. Hence, it is not subject to bias from either unmeasured confounders or feature redundancy. Example: Economy</p>
<ul class="simple">
<li><p>Independent from other features in dendogram (no observed confounding)</p></li>
<li><p>No unobserved confounding in causal digram</p></li>
</ul>
<p><strong>When can they not be used? (1) When you have observed confounding.</strong> Example: Ad Spend (no direct causal effect on retention, but correlated with Last upgrade and Monthly usage which do drive retention). ‘Our predictive model identifies Ad Spend as the one of the best single predictors of retention because it captures so many of the true causal drivers through correlations. XGBoost imposes regularization, which is a fancy way of saying that it tries to choose the simplest possible model that still predicts well. If it could predict equally well using one feature rather than three, it will tend to do that to avoid overfitting.’</p>
<p>However, there are methods to deal with observed confounding, such as double/debiased machine learning model. This involves:</p>
<ol class="arabic simple">
<li><p>Train model to predict feature (Ad spend) using set of confounders (features not caused by Ad spend)</p></li>
<li><p>Train model to predict outcome (Did Renew) using that set of confounders</p></li>
<li><p>Train model to predict residual variation of outcome (the variation left after subtracting our prediction) using the residual variation of the causal feature of interest</p></li>
</ol>
<p>‘The intuition is that if Ad Spend causes renewal, then the part of Ad Spend that can’t be predicted by other confounding features should be correlated with the part of renewal that can’t be predicted by other confounding features.’ There are packages like econML’s LinearDML for this.</p>
<p><strong>When can they not be used? (2) When you have non-confounding redundancy.</strong> ‘This occurs when the feature we want causal effects for causally drives, or is driven by, another feature included in the model, but that other feature is not a confounder of our feature of interest.’</p>
<p>Example: Sales Calls directly impact retention, but also have an indirect effect on retention through Interactions. We can see this in the SHAP scatter plots above, which show how XGBoost underestimates the true causal effect of Sales Calls because most of that effect got put onto the Interactions feature.</p>
<p>‘<strong>Non-confounding redundancy can be fixed in principle by removing the redundant variables from the model</strong> (see below). For example, if we removed Interactions from the model then we will capture the full effect of making a sales call on renewal probability. This removal is also important for double ML, since double ML will fail to capture indirect causal effects if you control for downstream features caused by the feature of interest. In this case double ML will only measure the “direct” effect that does not pass through the other feature. Double ML is however robust to controlling for upstream non-confounding redundancy (where the redundant feature causes the feature of interest), though this will reduce your statistical power to detect true effects. Unfortunately, we often don’t know the true causal graph so it can be hard to know when another feature is redundant with our feature of interest because of observed confounding vs. non-confounding redundancy. If it is because of confounding then we should control for that feature using a method like double ML, whereas if it is a downstream consequence then we should drop the feature from our model if we want full causal effects rather than only direct effects. Controlling for a feature we shouldn’t tends to hide or split up causal effects, while failing to control for a feature we should have controlled for tends to infer causal effects that do not exist. This generally makes controlling for a feature the safer option when you are uncertain.’</p>
<p><strong>When can they not be used? (3) When you have unobserved confounding.</strong> ‘The Discount and Bugs Reported features both suffer from unobserved confounding because not all important variables (e.g., Product Need and Bugs Faced) are measured in the data. Even though both features are relatively independent of all the other features in the model, there are important drivers that are unmeasured. In this case, both predictive models and causal models that require confounders to be observed, like double ML, will fail. This is why double ML estimates a large negative causal effect for the Discount feature even when controlling for all other observed features’</p>
<p>‘Specialized causal tools based on the principals of instrumental variables, differences-in-differences, or regression discontinuities can sometimes exploit partial randomization even in cases where a full experiment is impossible. For example, instrumental variable techniques can be used to identify causal effects in cases where we cannot randomly assign a treatment, but we can randomly nudge some customers towards treatment, like sending an email encouraging them to explore a new product feature. Difference-in-difference approaches can be helpful when the introduction of new treatments is staggered across groups. Finally, regression discontinuity approaches are a good option when patterns of treatment exhibit sharp cut-offs (for example qualification for treatment based on a specific, measurable trait like revenue over $5,000 per month).’</p>
<p><a class="reference external" href="https://shap.readthedocs.io/en/latest/example_notebooks/overviews/Be%20careful%20when%20interpreting%20predictive%20models%20in%20search%20of%20causal%20insights.html#">[source]</a></p>
</section>
<section id="confusion-and-controversy">
<h2>Confusion and controversy<a class="headerlink" href="#confusion-and-controversy" title="Link to this heading">#</a></h2>
<section id="reasons-for-confusion">
<h3>Reasons for confusion<a class="headerlink" href="#reasons-for-confusion" title="Link to this heading">#</a></h3>
<p>Causal inference can be confusing and controversial. Reasons for this are:</p>
<ul class="simple">
<li><p>Causally <strong>unrelated</strong> variables can be <strong>highly correlated</strong></p></li>
<li><p>Results may be reported in a way that is careful to avoid referring to any causal relationships, but it will often still <strong>naturally be read and interpreted as causal</strong></p></li>
<li><p>Even if there is a causal relationship, sometimes the <strong>direction is unclear</strong> - would need to carefully examine the temporal relationships between the variables<a class="reference external" href="https://www.coursera.org/learn/crash-course-in-causality/lecture/x4UMR/confusion-over-causality">[source]</a></p></li>
</ul>
</section>
<section id="confusion-in-the-literature">
<h3>Confusion in the literature<a class="headerlink" href="#confusion-in-the-literature" title="Link to this heading">#</a></h3>
<p>In practice, prediction and causation are <strong>commonly conflated</strong>. A review of observational studies found that 26% (46 / 180) observational cohort studies conflated between etiology and prediction -</p>
<ul class="simple">
<li><p>In causal studies, this was mainly due to selection of covariates based on their ability to predict <strong>without taking causal structure into account</strong>.</p></li>
<li><p>In prediction studies, this was mainly due to <strong>causal interpretation</strong> of covariates included in a prediction model.</p></li>
</ul>
</section>
</section>
<section id="concepts-and-principles">
<h2>Concepts and principles<a class="headerlink" href="#concepts-and-principles" title="Link to this heading">#</a></h2>
<section id="ladder-of-causality">
<h3>Ladder of causality<a class="headerlink" href="#ladder-of-causality" title="Link to this heading">#</a></h3>
<p>Judea Pearl proposed the ‘<strong>Ladder of Causality</strong>’ to categorise different levels of causal thinking, with increasing levels of difficulty.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Level</p></th>
<th class="head"><p>Typical activity</p></th>
<th class="head"><p>Typical questions</p></th>
<th class="head"><p>Examples</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>Association</strong></p></td>
<td><p>Seeing</p></td>
<td><p>What is?<br>How would seeing X change my belief in Y?</p></td>
<td><p>What does a symptom tell me about a disease?<br>What does a survey tell us about the election results?</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Intervention</strong></p></td>
<td><p>Doing</p></td>
<td><p>What if?<br>What if I do X?</p></td>
<td><p>What if I take aspirin, will my headachbe be cured?<br>What if we ban cigarettes?</p></td>
</tr>
<tr class="row-even"><td><p><strong>Counterfactuals</strong></p></td>
<td><p>Imagining, retrospection</p></td>
<td><p>Why?<br>Was it X that caused Y?<br> What if I had acted differently?</p></td>
<td><p>Was it aspirin that stopped my headache?<br>Would Kennedy be alive had Oswald not shot him?<br>What if I had not been smoking the past two years?</p></td>
</tr>
</tbody>
</table>
<p><a class="reference external" href="https://cacm.acm.org/magazines/2019/3/234929-the-seven-tools-of-causal-inference-with-reflections-on-machine-learning/fulltext?mobile=false">[source]</a></p>
<p>Difference between interventions and counterfactuals in this hierarchy:</p>
<ul class="simple">
<li><p>With interventions, you ask what will happen on average if you perform an action.</p></li>
<li><p>With counterfactuals, you ask what would have happened if you had performed a different action.</p></li>
<li><p>These two queries are mathematically distinct as they require different levels of information to be answered (counterfactuals need more information to be answered)’.<a class="reference external" href="https://stats.stackexchange.com/questions/379799/difference-between-rungs-two-and-three-in-the-ladder-of-causation">[source]</a></p></li>
</ul>
</section>
<section id="types-of-inference">
<h3>Types of inference<a class="headerlink" href="#types-of-inference" title="Link to this heading">#</a></h3>
<p>From C.S.Peirce in late 1800s:</p>
<ul class="simple">
<li><p>‘<strong>Deduction</strong> - necessary inference following logic’ <a class="reference external" href="https://www.statslab.cam.ac.uk/~qz280/teaching/causal-2023/notes-2021.pdf">[Zhao 2022]</a></p>
<ul>
<li><p>e.g. If dentist appointment at 10 and it’s 30 minute drive, deduce you need to leave at 9.30 <a class="reference external" href="https://www.merriam-webster.com/grammar/deduction-vs-induction-vs-abduction">[source]</a></p></li>
</ul>
</li>
<li><p>‘<strong>Induction</strong> - probable or non-necessary inference (purely) based on statistical data</p>
<ul>
<li><p>e.g. Correlation between cigarette smoking and lung cancer’ <a class="reference external" href="https://www.statslab.cam.ac.uk/~qz280/teaching/causal-2023/notes-2021.pdf">[Zhao 2022]</a></p></li>
<li><p>Four of your six coworker order the same sandwich so you induce that the sandwich is probably good <a class="reference external" href="https://www.merriam-webster.com/grammar/deduction-vs-induction-vs-abduction">[source]</a></p></li>
</ul>
</li>
<li><p>‘<strong>Abduction</strong> - inference with implicit or explicit appeal to explanatory considerations</p>
<ul>
<li><p>e.g. Investigation of a crime scene</p></li>
<li><p>Cigarette smoking causes lung cancer’ <a class="reference external" href="https://www.statslab.cam.ac.uk/~qz280/teaching/causal-2023/notes-2021.pdf">[Zhao 2022]</a></p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./causal_concepts"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="design_intro_1_treatment_paradox.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">The treatment paradox</p>
      </div>
    </a>
    <a class="right-next"
       href="design_intro_3_causal.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Causal inference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#how-do-you-know-whether-you-are-interested-in-prediction-or-causation">How do you know whether you are interested in prediction or causation?</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#illustrative-example">Illustrative example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explainability-v-s-causality">Explainability v.s. causality</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#doing-predictive-and-etiological-research">Doing predictive AND etiological research</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#when-can-prediction-models-answer-causal-questions">When can prediction models answer causal questions?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-and-controversy">Confusion and controversy</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#reasons-for-confusion">Reasons for confusion</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confusion-in-the-literature">Confusion in the literature</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#concepts-and-principles">Concepts and principles</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ladder-of-causality">Ladder of causality</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#types-of-inference">Types of inference</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Amy Heather
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>